{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from re import *\n",
    "import spacy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def get_text_segments(text: str):\n",
    "    '''\n",
    "    Extracts text segments from the given text by removing special characters and\n",
    "    splitting the text at punctuation marks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of cleaned text segments.\n",
    "    '''\n",
    "    \n",
    "    # If there is no content, return empty list\n",
    "    if isinstance(text, float):\n",
    "        return []\n",
    "    \n",
    "    # Convert text to lowercase and normalize\n",
    "    text_ = text.lower()\n",
    "    \n",
    "    # Clean special signs and normalize\n",
    "    text_ = re.sub('[!-&\\(-+/<=>@{-¿\\[-`÷€]', '', text_)\n",
    "    text_ = re.sub(\"[’'\\-]\", ' ', text_)\n",
    "    \n",
    "    # Replace multiple spaces\n",
    "    text_ = re.sub('[ ]+', ' ', text_)\n",
    "\n",
    "    # Split the text at each punctuation sign\n",
    "    segments = re.findall('([^!.,;:?«»\"\\(\\)–—]+)', text_)\n",
    "\n",
    "    # Remove spaces at start and end of the string\n",
    "    segments = [seg.strip() for seg in segments]\n",
    "\n",
    "    # Remove empty segments\n",
    "    segment_length = [len(seg) for seg in segments]\n",
    "    segments = np.array(segments)[np.array(segment_length) > 0]\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def unroll(nested_list: list):\n",
    "    '''\n",
    "    Small function used to flatten a nested list.\n",
    "    \n",
    "    Args:\n",
    "        nested_list (list): The input nested list.\n",
    "    \n",
    "    Returns:\n",
    "        list: A flattened list.\n",
    "    '''\n",
    "    return [item for list_ in nested_list for item in list_]\n",
    "\n",
    "def get_ngrams(text, n: int = 2):\n",
    "    '''\n",
    "    Extracts n-grams from the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        n (int): The size of the n-grams.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of n-grams.\n",
    "    '''\n",
    "    \n",
    "    # Compute speech segments (e.g. sentences)\n",
    "    segments = get_text_segments(text)\n",
    "    \n",
    "    # For each segment, iterate over the text and store the ngrams as tuples\n",
    "    ngrams = []\n",
    "    for segment in segments:\n",
    "        tokens = segment.split(' ')\n",
    "        ngrams.append([tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)])\n",
    "        \n",
    "    return unroll(ngrams)\n",
    "\n",
    "def count_ngrams(df, col: str = 'content', n: int = 1):\n",
    "    '''\n",
    "    Counts all the occurrences of n-grams in a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        col (str): The column name containing the text.\n",
    "        n (int): The length n of the n-grams.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: A DataFrame containing unique n-grams and their counts.\n",
    "    '''\n",
    "    \n",
    "    # Initialize timer\n",
    "    start = time.time()\n",
    "    \n",
    "    # Recover all n-grams and store them in a new column \n",
    "    df[f'{n}-gram'] = df['content'].apply(get_ngrams, n=n)\n",
    "    \n",
    "    # Find all unique n-grams and count the number of occurences\n",
    "    ngram_list = unroll(df[f'{n}-gram'].values.tolist())\n",
    "    unq, cnt = np.unique(ngram_list, return_counts=True, axis=0)\n",
    "    print(f'{len(ngram_list)} {n}-grams found in total, corresponding to {len(unq)} unique {n}-grams.')\n",
    "    \n",
    "    # Storing the results in a dataframe\n",
    "    unq = [tuple(it) for it in unq]\n",
    "    df_ngram = pd.DataFrame({'unique': unq, 'count': cnt}).sort_values(by='count', ascending=False)\n",
    "\n",
    "    print('Time elapsed:', time.time()-start)\n",
    "    \n",
    "    return df_ngram\n",
    "\n",
    "def count_ngram_occurrences(df, ngram: tuple):\n",
    "    '''\n",
    "    Count the occurrences of the given ngram in a dataframe and group the counts by year.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): The input dataframe containing at least n-gram columns and a 'year' column.\n",
    "        ngram (tuple): The n-gram tuple to be counted in the dataframe.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: A new dataframe with n-gram occurrences grouped by year.\n",
    "    '''\n",
    "    \n",
    "    def count_ngram(ngrams, ngram):\n",
    "        '''\n",
    "        Counts the number of times where the given ngram appears in a list of ngrams.\n",
    "        '''\n",
    "        vector = np.empty(len(ngrams), dtype=object)\n",
    "        vector[:] = ngrams\n",
    "\n",
    "        return np.sum(vector == ngram)\n",
    "    \n",
    "    if isinstance(ngram, str):\n",
    "        len_ngram = 1\n",
    "        ngram = (ngram,)\n",
    "    else:\n",
    "        len_ngram = len(ngram)\n",
    "    \n",
    "    # Create a numpy array with the ngram to count\n",
    "    item = np.empty(1, dtype=object)\n",
    "    item[:] = [ngram]\n",
    "\n",
    "    # Count the number of occurences of the ngram (item) in each article\n",
    "    df['occurences'] = df[f'{len_ngram}-gram'].apply(count_ngram, ngram=item)\n",
    "\n",
    "    # Store the output in a new dataframe grouped by year\n",
    "    df_ngram_by_year = df[['occurences', 'year']].groupby(by='year').sum()\n",
    "    df_ngram_by_year = df_ngram_by_year.rename(columns = {'occurences': str(ngram)})\n",
    "\n",
    "    # Reset df dataframe\n",
    "    df = df.drop(columns = ['occurences'])\n",
    "    \n",
    "    return df_ngram_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" /!\\ il faut définir l import impresso à utiliser /!\\ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load('fr_core_news_lg')\n",
    "df = pd.read_csv('.\\export-2023-05-02-10429\\guisan.csv', sep=';')\n",
    "articles = df['content'].dropna().values.tolist()[:500]\n",
    "\n",
    "documents, tokens = [], []\n",
    "for article in articles:\n",
    "    words = findall('([a-zà-ÿ]+)', article.lower())\n",
    "    tokens += words\n",
    "    documents.append(words)\n",
    "\n",
    "vocabulaire = pd.Series(tokens).value_counts()[20:]\n",
    "vocabulaire = vocabulaire [vocabulaire >= 3].sort_index().keys()\n",
    "\n",
    "one_hot = np.zeros((len(documents), len(vocabulaire)))\n",
    "for index, document in enumerate(documents):\n",
    "    for token in document:\n",
    "        one_hot[index][int(np.argmax(vocabulaire == token))] = 1\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=7, random_state=0) # il faut jouer sur le nombre de clusters mais jsp comment\n",
    "res = lda.fit_transform(one_hot)\n",
    "\n",
    "# Mots les plus fréquents du cluster 5\n",
    "for i in np.flip(np.argsort(lda.components_[5]))[:20]:\n",
    "    print(vocabulaire[i], end=', ')\n",
    "\n",
    "# Mots les plus caractéristiques du cluster 5\n",
    "type_attribution = lda.transform(np.identity(len(vocabulaire)))\n",
    "for t in vocabulaire[np.flip(np.argsort(type_attribution[:,5]))[:20]]:\n",
    "    print(t, end=', ')\n",
    "\n",
    "#les articlesles plus represenatifs\n",
    "for art_id in np.flip(np.argsort(res[:, 5]))[:3]:\n",
    "    print(articles[art_id], end='\\n\\n')\n",
    "\n",
    "data_ngrams = {}\n",
    "\n",
    "for n in range(1, 6):\n",
    "    data_ngrams[n] = count_ngrams(df, n=n)\n",
    "\n",
    "\n",
    "data_ngrams[3].head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ngrams[5].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_ngrams[4].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" /!\\il faut définir le n-gram à utiliser /!\\ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Observe and display the temporal evolution of a specific ngram\n",
    "ngram = ('commandant', 'du', '1', 'er', 'corps')\n",
    "df_ngram_by_year = count_ngram_occurrences(df, ngram)\n",
    "\n",
    "# Customize the appearance of the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6), dpi=200)\n",
    "df_ngram_by_year.plot(ax=ax, linestyle='-')\n",
    "\n",
    "# Add labels, a title, gridlines, and a legend\n",
    "ax.set_xlabel('Années', fontsize=14)\n",
    "ax.set_ylabel('Occurrences', fontsize=14)\n",
    "ax.set_title(f'Evolution temporelle par n-gram', fontsize=16)\n",
    "ax.legend([f'{\" \".join(ngram)}'], loc='best', fontsize=12)\n",
    "\n",
    "# Format the y-axis ticks\n",
    "ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find sentences containing the ngram and the sentence before and after\n",
    "def find_sentences(articles, ngram: tuple):\n",
    "    W=[]\n",
    "    for article in articles:\n",
    "        if \" \".join(ngram) in article.lower():\n",
    "            print(articles.index(article))\n",
    "            print(article + '\\n\\n')\n",
    "            words = re.findall(r'[^.?!]*\\. '+r'[^.?!]*\\ ' +' '.join(ngram)+r' [^.?!]*\\. '+r'[^.?!]*\\.', article.lower())\n",
    "            if words==[]:\n",
    "                words=re.findall(r'[^.?!]*\\. '+r'[^.?!]*\\ ' +' '.join(ngram)+r' [^.?!]*\\.', article.lower()) #si c'est dans une dernière phrase\n",
    "                if words==[]:\n",
    "                    words=re.findall(r'[^.?!]*\\ ' +' '.join(ngram)+r' [^.?!]*\\. '+r'[^.?!]*\\.', article.lower())#si c'est dans une première phrase\n",
    "                    if words==[]:\n",
    "                        words=re.findall(' '.join(ngram)+r' [^.?!]*\\. '+r'[^.?!]*\\.', article.lower())#si c est le debut d'une première phrase\n",
    "            \n",
    "            for word in words:\n",
    "               index=article.lower().index(word)\n",
    "            W.append(article[index: index+len(word)])\n",
    "    return W\n",
    "\n",
    "articles = df['content'].dropna().values.tolist()\n",
    "seg=find_sentences(articles, ('commandant', 'du', '1', 'er', 'corps'))\n",
    "print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
